{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we cover the topic of `overfitting`. A folk theorem is asset management is that people are so afraid of overfitting that they tend to (massively) underfit. Or at least, that was the case. Today, better fitting models to extract as much information from a dataset has become a crucial skill. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide \n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import display, Image\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from lightgbm.sklearn import LGBMRegressor\n",
    "\n",
    "from ml4pmt.plot import line, bar\n",
    "from ml4pmt.dataset import load_kf_returns\n",
    "from ml4pmt.backtesting import MeanVariance, Backtester\n",
    "from ml4pmt.metrics import sharpe_ratio\n",
    "from ml4pmt.estimators import RidgeCV, MultiOutputRegressor, MLPRegressor\n",
    "\n",
    "returns_data = load_kf_returns(cache_dir='data')\n",
    "ret = returns_data['Monthly']['Average_Value_Weighted_Returns'][:'1999']\n",
    "\n",
    "transform_X = lambda x: x.rolling(12).mean().fillna(0).values\n",
    "transform_y = lambda x: x.shift(-1).values\n",
    "features = transform_X(ret)\n",
    "target = transform_y(ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random parameter search for Lightgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://lightgbm.readthedocs.io/en/latest/Parameters.html#learning-control-parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first compute a `Lightgbm` benchmark with the fixed baseline parameters used in a previous section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = make_pipeline(MultiOutputRegressor(LGBMRegressor(min_child_samples=5, \n",
    "                                                             n_estimators=25, n_jobs=1)), \n",
    "                          MeanVariance())\n",
    "\n",
    "pnl_lgb = {'fixed_params':  Backtester(estimator, ret).train(features, target).pnl_ }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now do a search with random parameters drawn from predetermined distribution: \n",
    "- the random parameter generators come from the `scipy.stats` module -- in particular `randint`, `uniform` and `loguniform`.\n",
    "- we use the `scikit-learn` function `ParameterSampler` as wrapper. \n",
    "\n",
    "Setup: \n",
    "- the objective is to maximize the sharpe ratio over the early period 1945 to 1972 (as the `train` period). \n",
    "- the evaluation is the performance of the backtest over the 1972-to-2000 period (as the `test` period). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterSampler\n",
    "from scipy.stats import randint, uniform, loguniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 50 \n",
    "start_date = \"1945-01-01\"\n",
    "end_date = \"1972-04-01\"\n",
    "param_distributions = {\"max_depth\": randint(3, 10),\n",
    "                       \"num_leaves\": randint(2, 2**8), \n",
    "                       \"n_estimators\": randint(5, 50), \n",
    "                       \"min_split_gain\": uniform(0, 1.0), \n",
    "                       \"min_child_samples\": randint(1, 5), \n",
    "                       \"reg_lambda\": loguniform(1e-8, 1.0), \n",
    "                       \"reg_alpha\": loguniform(1e-8, 1.0)\n",
    "                      }\n",
    "results_ = {}\n",
    "for i, prm in tqdm(enumerate(ParameterSampler(param_distributions=param_distributions, n_iter=n_iter))): \n",
    "    estimator = make_pipeline(MultiOutputRegressor(LGBMRegressor(n_jobs=1, **prm)), \n",
    "                              MeanVariance())\n",
    "    pnl_ = Backtester(estimator, ret, end_date=end_date).train(features, target).pnl_\n",
    "    prm.update({'sr': pnl_.pipe(sharpe_ratio)})\n",
    "    results_[i] = pd.Series(prm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame.from_dict(results_, orient='index').sort_values('sr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = results.sort_index()[['sr']].assign(sr_cummax = lambda x: x.sr.cummax())\n",
    "line(df, title='Optimisation history: random search')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the sharpe ratio statistics presented in a previous section, we can compute a standard error around the maximum sharpe ratio: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr_max = results.iloc[-1]['sr'] / np.sqrt(12)\n",
    "sr_std = np.sqrt(12) * np.sqrt((1 + .5 * sr_max **2)/ len(ret[start_date:end_date])) \n",
    "sr_range = results['sr'].pipe(lambda x: x.max()-x.min())\n",
    "print(f'The sharpe ratio standard deviation at the maximum sharpe ratio (of {sr_max * np.sqrt(12):.2f}) is {sr_std:.2f}')\n",
    "\n",
    "print(f'The range of the sharpe ratios in the random search is {sr_range:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = results.drop('sr', axis=1).iloc[-1].to_dict()\n",
    "best_params['num_leaves'] = int(best_params['num_leaves'])\n",
    "best_params['max_depth'] = int(best_params['max_depth'])\n",
    "best_params['min_child_samples'] = int(best_params['min_child_samples'])\n",
    "best_params['n_estimators'] = int(best_params['n_estimators'])\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = make_pipeline(MultiOutputRegressor(LGBMRegressor(n_jobs=1, **best_params)), \n",
    "                          MeanVariance())\n",
    "pnl_lgb['best_params'] = Backtester(estimator, ret).train(features, target).pnl_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line({k: v.loc[start_date:end_date] for k, v in pnl_lgb.items()}, cumsum=True, title='Lightgbm search: in-sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line({k: v.loc[end_date:] for k, v in pnl_lgb.items()}, cumsum=True, title='Lightgbm search: out-of-sample')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the parameters that are correlated with the sharpe ratio? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar(results.corr()['sr'].mul(np.sqrt(n_iter)).drop('sr'), \n",
    "    title='T-stat correlation param value / sharpe ratio', horizontal=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To assess more precisely the impact of parameters on the sharpe ratio, we run a regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels import api\n",
    "m = api.OLS(results[\"sr\"], api.add_constant(results.drop('sr', axis=1))).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another strategy is to use estimators that embed some form of cross-validation like `RidgeCV`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-fold cross validation is described as follows: \n",
    "\n",
    "Take a model with parameter $s$ (e.g. the Ridge with tuning parameter `alpha`): \n",
    "\n",
    "1. divide the data into $K$ roughly equal parts ($K = 5$ or $K = 10$)\n",
    "\n",
    "1. for each $k \\in \\{1, 2,..., K\\}$ fit the model with parameter $s$ to the other $K-1$ parts and compute its error $E_k(s)$  in predicting the $k$-th part.\n",
    "\n",
    "1. the overall cross-validation error is then $CV(s)= \\frac{1}{K} \\sum_{k=1}^K E_k(s)$. \n",
    "\n",
    "1. do this for many values of $s$ and choose the value of s that minimize $CV (s)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.exp(np.arange(np.log(10),np.log(10001), (np.log(10000) - np.log(10))/10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = make_pipeline(StandardScaler(with_mean=False), \n",
    "                          PolynomialFeatures(degree=2), \n",
    "                          RidgeCV(alphas=alphas, cv=5), \n",
    "                          MeanVariance())\n",
    "\n",
    "m = Backtester(estimator, ret).train(features, target)\n",
    "line(m.pnl_, cumsum=True, title='RidgeCV with polynomial features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line(pd.Series([m[2].alpha_ for m in m.estimators_]), title='Cross-validated alphas', legend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the fitted alphas over rolling windows are not very stable (probably given the small rolling windows used here)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than choosing a single estimator (or set of parameters) among many, another stategy is to combine all the possible estimators/parameters. `scikit-learn` allows to do that with classes such as `VotingRegressor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml4pmt.estimators import Ridge\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "class VotingRegressor(VotingRegressor):\n",
    "    def transform(self, X):\n",
    "        return self.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators_ = [('ridge1', Ridge(alpha=1)),  \n",
    "               ('ridge2', Ridge(alpha=100)), ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, `VotingRegressor` applies equal weights across regressors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = make_pipeline(StandardScaler(with_mean=False), \n",
    "                          MultiOutputRegressor(VotingRegressor(estimators=estimators_)), \n",
    "                          MeanVariance())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Backtester(estimator, ret).train(features, target)\n",
    "line(m.pnl_, cumsum=True, title='Voting regressor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `scikit-learn`, there is also a `StackingRegressor` but it requires a bit more work to make it work with `MultiOutputRegressor` (and constraints on transform/regressors). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rolling ensemble backtest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we build a custom ensemble method to learn weights on different estimators from pnls. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A shortcut to compute markowitz weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "display(Image('images/mbj.png',width=500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A final strategy is to learn weights based on pnl over rolling windows (longer than the estimation windows)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trick to compute markowitz weights just with the pnl of different assets \n",
    "\n",
    "- X: pnl of $K$ assets over $T$ days -- so that the shape of X is $[T \\times K]$. \n",
    "\n",
    "- y: vector of ones of size $T$. \n",
    "\n",
    "**Lemma** [Mark Britten-Jones]: the markowitz weights of are proportional to the slope coefficient of a regression of the vector of ones $y$ on the pnls $X$ *with no intercept*. \n",
    "\n",
    "Proof: the coefficient of the regression with no intercept is given by \n",
    "\n",
    "$$ b = (X^T X)^{-1} X^T y  $$\n",
    "\n",
    "The mean of the pnls is given by $\\mu = \\frac{1}{T} X^T y$. The variance of the pnls is $V = \\frac{1}{T} X^T X - \\mu \\mu^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Woodbury identity (https://en.wikipedia.org/wiki/Woodbury_matrix_identity), we have: \n",
    "\n",
    "$$ b = (V + \\mu \\mu^{T})^{-1} \\mu = \\left[ V^{-1} -  \\frac{V^{-1} \\mu \\mu^{T}V^{-1}}{1 + \\mu^T V^{-1} \\mu}  \\right] \\mu = \\frac{V^{-1} \\mu}{1 + \\mu^T V^{-1} \\mu} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main trick is to recognise that \n",
    "\n",
    "$$(X^T X)^{-1} X^T y \\propto [X^T X - (X^T y)^T  X^T y ]^{-1} X^T y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StackingBacktester"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we consider three estimators: \n",
    "    \n",
    "- the simple Industry momentum. \n",
    "\n",
    "- a strategy that learns cross-industry effect with `Ridge`. \n",
    "\n",
    "- a strategy that learns cross-industry effect with `Lightgbm`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = {'momentum': MeanVariance(), \n",
    "              'ridge':  make_pipeline(StandardScaler(with_mean=False), Ridge(), MeanVariance()), \n",
    "              'lightgbm': make_pipeline(MultiOutputRegressor(LGBMRegressor(min_child_samples=5, \n",
    "                                                             n_estimators=25, n_jobs=1)), MeanVariance())\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pnls = pd.concat({k: Backtester(v, ret).train(features, target).pnl_ for k, v in estimators.items()}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pnls_ = pnls.assign(equal_weight = lambda x: x.sum(axis=1).div(np.sqrt(x.shape[1])))\n",
    "line(pnls_, cumsum=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average correlation is not particularly high, which explains with some simple ensemble seems to help. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The average pnl correlation between estimators is {pnls.corr().stack().loc[lambda x: x!=1].mean():.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We introduce a simple `MBJ` estimator and `StackingBacktester` with the `sklearn` api. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../ml4pmt/ensemble.py\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "class Mbj(TransformerMixin): \n",
    "    def __init__(self, positive=False): \n",
    "        self.positive=positive \n",
    "        \n",
    "    def fit(self, X, y=None): \n",
    "        m = LinearRegression(fit_intercept=False, positive=self.positive)\n",
    "        m.fit(X, y = np.ones(len(X)))\n",
    "        self.coef_ = m.coef_ / np.sqrt(np.sum(m.coef_**2))\n",
    "        return self\n",
    "\n",
    "    def transform(self, X): \n",
    "        return X.dot(self.coef_)\n",
    "    \n",
    "\n",
    "class StackingBacktester:\n",
    "    def __init__(\n",
    "        self,\n",
    "        estimators,\n",
    "        ret,\n",
    "        max_train_size=36,\n",
    "        test_size=1,\n",
    "        start_date=\"1945-01-01\",\n",
    "        end_date=None,\n",
    "        window=60, \n",
    "        min_periods=60, \n",
    "        final_estimator = Mbj()\n",
    "    ):\n",
    "\n",
    "        self.start_date = start_date\n",
    "        self.end_date = end_date\n",
    "        self.estimators = estimators\n",
    "        self.ret = ret[: self.end_date]\n",
    "        self.cv = TimeSeriesSplit(\n",
    "            max_train_size=max_train_size,\n",
    "            test_size=test_size,\n",
    "            n_splits=1 + len(ret.loc[start_date:end_date]) // test_size,\n",
    "        )\n",
    "        self.window = window\n",
    "        self.min_periods = min_periods\n",
    "        self.final_estimator = final_estimator \n",
    "\n",
    "    def train(self, features, target):\n",
    "        cols =self.ret.columns \n",
    "        idx = self.ret.index[np.concatenate([test for _, test in self.cv.split(self.ret)])]\n",
    "\n",
    "        _h = {k: [] for k in list(self.estimators.keys()) + ['ensemble']}\n",
    "        _pnls = {k: [] for k in self.estimators.keys()}\n",
    "        _coef = []\n",
    "        for i, (train, test) in enumerate(self.cv.split(self.ret)): \n",
    "            h_ = {}\n",
    "            if (i> self.min_periods): \n",
    "                pnl_window = np.stack([np.array(v[-self.window:]) for k, v in _pnls.items()], axis=1)\n",
    "                coef_ = self.final_estimator.fit(pnl_window).coef_\n",
    "                _coef += [coef_]\n",
    "            else: \n",
    "                _coef += [np.zeros(3)] \n",
    "            for k, m in self.estimators.items(): \n",
    "                m.fit(features[train], target[train])\n",
    "                h_[k] = m.predict(features[test])\n",
    "                _h[k] += [h_[k]]\n",
    "                if i+1 <len(idx):\n",
    "                    _pnls[k] += [self.ret.loc[idx[i+1]].dot(np.squeeze(h_[k]))]\n",
    "            if (i>self.min_periods): \n",
    "                h_ensemble = np.stack([np.squeeze(v) for v in h_.values()], axis=1).dot(coef_).reshape(-1, 1)\n",
    "                V_ = m.named_steps['meanvariance'].V_\n",
    "                h_ensemble = h_ensemble / np.sqrt(np.diag(h_ensemble.T.dot(V_.dot(h_ensemble))))\n",
    "            else: \n",
    "                h_ensemble = np.zeros([len(cols), 1])\n",
    "            _h['ensemble'] += [h_ensemble.T]\n",
    "            \n",
    "        self.h_ = {k: pd.DataFrame(np.concatenate(_h[k]), index=idx, columns=cols) \n",
    "                   for k in _h.keys()}\n",
    "        self.pnls_ = pd.concat({k: v.shift(1).mul(self.ret).sum(axis=1)[self.start_date:] \n",
    "                                for k, v in self.h_.items()}, \n",
    "                               axis=1)\n",
    "        self.coef_ = pd.DataFrame(np.stack(_coef), index=idx, columns=self.estimators.keys())\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml4pmt.ensemble import Mbj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Mbj()\n",
    "m.fit(pnls)\n",
    "bar(pd.Series(m.coef_, index=pnls.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The in-sample optimal weights improve even more the sharpe ratio -- but this is `in-sample`! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line(pnls_.assign(in_sample_optimal = Mbj().fit_transform(pnls)), cumsum=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `StackingBacktester` computes the performance with the `MBJ` learned weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml4pmt.ensemble import StackingBacktester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = StackingBacktester(estimators=estimators, \n",
    "                       ret=ret, window=60,min_periods=60).train(features, target)\n",
    "pnls = pnls.assign(ensemble_mbj= m.pnls_['ensemble'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line(m.pnls_['1950-02-01':], cumsum=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand why the performance is lower, it is useful to look at the weights -- in this case, the weights are often negative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line(m.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We redo the exercise with a positive-weight constraint. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = StackingBacktester(estimators=estimators, \n",
    "                       final_estimator=Mbj(positive=True), \n",
    "                       ret=ret, \n",
    "                       window=60,\n",
    "                       min_periods=60)\n",
    "m.train(features, target)\n",
    "pnls['ensemble_mbj_positive'] = m.pnls_['ensemble']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(20, 5))\n",
    "line(m.pnls_['1950-02-01':], cumsum=True, ax=ax[0], loc='best')\n",
    "line(m.coef_, ax=ax[1], loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over longer periods with positive constraints, the performance is closer to the industry momentum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = StackingBacktester(estimators=estimators, \n",
    "                       final_estimator=Mbj(positive=True), \n",
    "                       ret=ret, window=180,min_periods=60)\n",
    "\n",
    "m.train(features, target)\n",
    "pnls['ensemble_mbj_positive_long_window'] = m.pnls_['ensemble']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(20, 5))\n",
    "line(m.pnls_, cumsum=True, ax=ax[0], loc='best')\n",
    "line(m.coef_, ax=ax[1], loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting the different ensembles, we compare the pnls in the graph below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line(pnls['1950-02-01':], cumsum=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "teaching",
   "language": "python",
   "name": "teaching"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
